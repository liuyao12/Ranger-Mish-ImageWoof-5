{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Woof_128_twist.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LP3vVSWMBz1L",
        "6C-1pE45Bz1l",
        "pLmt0ISPBz2w"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liuyao12/Ranger-Mish-ImageWoof-5/blob/master/Woof_128_ResNet_with_a_Twist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM0cNcW2Bzz4",
        "colab_type": "text"
      },
      "source": [
        "# ResNet with a Twist\n",
        "\n",
        "> ConvTwist + Ranger + Mish + MaxBlurPool + restrick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrD1-te_Bzz7",
        "colab_type": "text"
      },
      "source": [
        "# setup and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtfoQmHrJonm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pip install git+https://github.com/kornia/kornia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UteAROmqBzz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "d2b5406e-4e17-437d-8bc6-74db624a3c61"
      },
      "source": [
        "pip install git+https://github.com/ayasyrev/model_constructor"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ayasyrev/model_constructor\n",
            "  Cloning https://github.com/ayasyrev/model_constructor to /tmp/pip-req-build-an56dx0j\n",
            "  Running command git clone -q https://github.com/ayasyrev/model_constructor /tmp/pip-req-build-an56dx0j\n",
            "Collecting fastcore\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/6e/a18c0ff6cdca36915e65cf1690137134241a33d74ceef7882f4a63a6af55/fastcore-0.1.18-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from model-constructor==0.1.1) (1.5.1+cu101)\n",
            "Requirement already satisfied: dataclasses>='0.7'; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastcore->model-constructor==0.1.1) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore->model-constructor==0.1.1) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->model-constructor==0.1.1) (0.16.0)\n",
            "Building wheels for collected packages: model-constructor\n",
            "  Building wheel for model-constructor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for model-constructor: filename=model_constructor-0.1.1-cp36-none-any.whl size=23583 sha256=043d465e5ca1b027b2e487f65e2ba235a1620926dea00f99c7d9fb8fc2bdb08d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oe3xheo2/wheels/5b/92/65/8093a307d5802f41f4a8776b40bc12b558e75a2a906ae8b683\n",
            "Successfully built model-constructor\n",
            "Installing collected packages: fastcore, model-constructor\n",
            "Successfully installed fastcore-0.1.18 model-constructor-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XnsPVNrkTfV3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b9efe88-5080-4377-f307-a4a450fca0aa"
      },
      "source": [
        "pip install git+https://github.com/ayasyrev/imagenette_experiments"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ayasyrev/imagenette_experiments\n",
            "  Cloning https://github.com/ayasyrev/imagenette_experiments to /tmp/pip-req-build-7h5c6bpf\n",
            "  Running command git clone -q https://github.com/ayasyrev/imagenette_experiments /tmp/pip-req-build-7h5c6bpf\n",
            "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (from imagenette-experiments==0.0.1) (1.0.61)\n",
            "Collecting kornia\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/60/f0c174c4a2a40b10b04b37c43f5afee3701cc145b48441a2dc5cf9286c3c/kornia-0.3.1-py2.py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: model-constructor in /usr/local/lib/python3.6/dist-packages (from imagenette-experiments==0.0.1) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (1.5.1+cu101)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (3.13)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (1.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (2.7.1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (1.18.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (7.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (1.0.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (0.2.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (0.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (4.6.3)\n",
            "Requirement already satisfied: spacy>=2.0.18; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (2.2.4)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (7.352.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (0.6.1+cu101)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai->imagenette-experiments==0.0.1) (2.23.0)\n",
            "Requirement already satisfied: fastcore in /usr/local/lib/python3.6/dist-packages (from model-constructor->imagenette-experiments==0.0.1) (0.1.18)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->fastai->imagenette-experiments==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai->imagenette-experiments==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai->imagenette-experiments==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai->imagenette-experiments==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai->imagenette-experiments==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai->imagenette-experiments==0.0.1) (2018.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (47.3.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (1.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai->imagenette-experiments==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai->imagenette-experiments==0.0.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai->imagenette-experiments==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai->imagenette-experiments==0.0.1) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai->imagenette-experiments==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18; python_version < \"3.8\"->fastai->imagenette-experiments==0.0.1) (3.1.0)\n",
            "Building wheels for collected packages: imagenette-experiments\n",
            "  Building wheel for imagenette-experiments (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagenette-experiments: filename=imagenette_experiments-0.0.1-cp36-none-any.whl size=15332 sha256=ac7f3fea547500cb6062701bb10ca0eed42597ff35ed157c559f82d2ee104944\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ocbjulaj/wheels/af/99/98/2212941f45a18bf6d191f56c39e3569138414560c76defc0d4\n",
            "Successfully built imagenette-experiments\n",
            "\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.5.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: kornia, imagenette-experiments\n",
            "Successfully installed imagenette-experiments-0.0.1 kornia-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-OCjxGCBz0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imagenette_experiments.train_utils import *"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPNF3XKWBz0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kornia.contrib import MaxBlurPool2d"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPGwrk6TBz0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.basic_train import *\n",
        "from fastai.vision import *\n",
        "# from fastai.script import *\n",
        "from model_constructor.net import Net, act_fn\n",
        "from model_constructor.layers import SimpleSelfAttention, ConvLayer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iLuG-qpBz0c",
        "colab_type": "text"
      },
      "source": [
        "# Twist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uDQr2mVcTZzB",
        "colab": {}
      },
      "source": [
        "class ConvTwist(nn.Module):  # replacing 3x3 Conv2d with in_channels=out_channels\n",
        "    def __init__(self, ni, nf):\n",
        "        super(ConvTwist, self).__init__()\n",
        "        self.twist = True and ni<=256\n",
        "        self.conv = nn.Conv2d(ni, nf, kernel_size=3, padding=1, bias=False, groups=ni)\n",
        "        if self.twist:\n",
        "            Dx = torch.Tensor([[-1,0,1],[-2,0,2],[-1,0,1]]).view(1,1,3,3)\n",
        "            Dy = torch.Tensor([[1,2,1],[0,0,0],[-1,-2,-1]]).view(1,1,3,3)\n",
        "            self.DD = torch.cat([Dx,Dy]*ni, dim=0)\n",
        "            a = 2\n",
        "            self.expansion = a\n",
        "            self.conv1 = nn.Conv2d(ni, ni*a, kernel_size=3, padding=1, bias=False, groups=ni)\n",
        "            self.conv2 = nn.Conv2d(ni*a, ni*a*2, kernel_size=1, bias=False, groups=ni*a)\n",
        "            # self.conv3 = nn.Conv2d(ni*a*2, nf*2, kernel_size=1, bias=False, groups=1)\n",
        "            self.XY = None\n",
        "        else:\n",
        "            self.conv = nn.Conv2d(ni, nf, kernel_size=3, padding=1, bias=False, groups=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        N,C,H,W = x.size()\n",
        "        # out = x\n",
        "        if self.twist:\n",
        "            CC = C*self.expansion\n",
        "            x_double = x.unsqueeze(2).expand(N,C,2,H,W).reshape(N,C*2,H,W)\n",
        "            if self.XY is None:\n",
        "                XX = torch.from_numpy(np.indices((1,1,H,W))[3]*2/W-1)\n",
        "                YY = torch.from_numpy(np.indices((1,1,H,W))[2]*2/H-1)\n",
        "                self.XY = torch.cat([XX,YY]*C, dim=1).type(x.dtype).to(x.device)\n",
        "                print(self.XY.size())\n",
        "                self.DD = self.DD.to(x.device)\n",
        "            x_plus = F.relu(x_double)+0.01\n",
        "            total = x_plus.sum((2,3))\n",
        "            XYbar = (self.XY*x_plus).sum((2,3))/total\n",
        "            XY = self.XY-XYbar[:,:,None,None]\n",
        "            # XY = XY.unsqueeze(2).expand(N,CC,2,H,W).reshape(N,CC*2,H,W)\n",
        "            XY = XY.view(N,C,2,H,W)\n",
        "            XY = torch.cat([XY,XY], dim=2).view(N,C*4,H,W)\n",
        "            out = F.conv2d(x, self.DD, padding=1, groups=C)\n",
        "            # out = self.conv1(x)\n",
        "            out = self.conv2(out)\n",
        "            # out = self.conv3(XY*out)\n",
        "            # N,CC,H,W = out.size()\n",
        "            # out = out.view(N,CC//2,2,H,W).sum(2)\n",
        "            out = self.conv(x) + XY * out\n",
        "        else:\n",
        "            out = self.conv(x)\n",
        "        return out"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51i_Xqjen7lT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9df334b0-cc0e-4f66-bed8-7f8fbb04193a"
      },
      "source": [
        "A = torch.arange(12).view(3,4)\n",
        "print(A.size())\n",
        "A.unsqueeze_(1)\n",
        "print(A.size())\n",
        "A = A.expand((3,2,4)).reshape(6,4)\n",
        "print(A)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([3, 1, 4])\n",
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11],\n",
            "        [ 8,  9, 10, 11]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U63Mcb1lBz0p",
        "colab_type": "text"
      },
      "source": [
        "# ResBlock"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY7y9c99Bz0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewLayer(nn.Sequential):\n",
        "    \"\"\"Basic conv layers block\"\"\"\n",
        "    def __init__(self, ni, nf, ks=3, stride=1,\n",
        "            act=True,  act_fn=nn.ReLU(inplace=True),\n",
        "            bn_layer=True, bn_1st=True, zero_bn=False,\n",
        "            padding=None, bias=False, groups=1, **kwargs):\n",
        "\n",
        "        if padding==None: padding = ks//2\n",
        "        if ks==3:  \n",
        "          layers = [('ConvTwist', ConvTwist(ni, nf))]\n",
        "          act = False\n",
        "        else: layers = [('Conv{}x{}'.format(ks,ks), \n",
        "                  nn.Conv2d(ni, nf, ks, stride=stride, padding=padding, bias=bias, groups=groups))]\n",
        "\n",
        "        act_bn = [('act_fn', act_fn)] if act else []\n",
        "        if bn_layer:\n",
        "            bn = nn.BatchNorm2d(nf)\n",
        "            nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
        "            act_bn += [('bn', bn)]\n",
        "        if bn_1st: act_bn.reverse()\n",
        "        layers += act_bn\n",
        "        super().__init__(OrderedDict(layers))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVLFz9nhBz0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NewResBlock(Module):\n",
        "    def __init__(self, expansion, ni, nh, stride=1,\n",
        "                 conv_layer=ConvLayer, act_fn=act_fn, bn_1st=True,\n",
        "                 pool=nn.AvgPool2d(2, ceil_mode=True), sa=False, sym=False, zero_bn=True):\n",
        "        nf,ni = nh*expansion,ni*expansion\n",
        "        conv_layer = NewLayer\n",
        "        self.reduce = noop if stride==1 else pool\n",
        "        layers  = [(f\"conv_0\", conv_layer(ni, nh, 3, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   (f\"conv_1\", conv_layer(nh, nf, 3, zero_bn=zero_bn, act=False, bn_1st=bn_1st))\n",
        "        ] if expansion == 1 else [\n",
        "                   (f\"conv_0\", conv_layer(ni, nh, 1, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   (f\"conv_1\", conv_layer(nh, nh*4, 3, act_fn=act_fn, bn_1st=bn_1st)),\n",
        "                   (f\"conv_2\", conv_layer(nh*4, nf, 1, zero_bn=zero_bn, act=False, bn_1st=bn_1st))\n",
        "        ]\n",
        "        if sa: layers.append(('sa', SimpleSelfAttention(nf,ks=1,sym=sym)))\n",
        "        self.convs = nn.Sequential(OrderedDict(layers))\n",
        "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False, bn_1st=bn_1st)\n",
        "        self.merge = act_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.reduce(x)\n",
        "        return self.merge(self.convs(o) + self.idconv(o))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XroIp4GcBz07",
        "colab_type": "text"
      },
      "source": [
        "# Model Constructor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTrZVV81Bz1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Net(c_out=10, layers=[3,6,8,3], expansion=4)\n",
        "model.block = NewResBlock\n",
        "# model.conv_layer = NewLayer # for the stem\n",
        "pool = MaxBlurPool2d(3, True)\n",
        "model.pool = pool\n",
        "model.stem_pool = pool\n",
        "model.stem_sizes = [3,32,64,64]\n",
        "model.act_fn = Mish()\n",
        "model.sa = True\n",
        "res = []"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g2J_rBABz13",
        "colab_type": "text"
      },
      "source": [
        "# Runs and results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QXtp7Jryx_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "25850a3f-8d32-4ae9-97cf-100a1b3c7449"
      },
      "source": [
        "epochs = [5]\n",
        "for e in epochs:\n",
        "    mixup=0 if e<20 else 0.2\n",
        "    learn = get_learn(model=model, size=192, bs=32, mixup=mixup)\n",
        "    learn.fit_fc(e, lr=4e-3, moms=(0.95,0.95), start_pct=0.72)\n",
        "    res += [learn.recorder.metrics[-1][0].item()]\n",
        "print([round(x, 6) for x in res], sum(res)/len(res))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.820164</td>\n",
              "      <td>1.688073</td>\n",
              "      <td>0.482566</td>\n",
              "      <td>0.889794</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.559968</td>\n",
              "      <td>1.375441</td>\n",
              "      <td>0.629168</td>\n",
              "      <td>0.950115</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.394244</td>\n",
              "      <td>1.286588</td>\n",
              "      <td>0.677272</td>\n",
              "      <td>0.958259</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.243194</td>\n",
              "      <td>1.129009</td>\n",
              "      <td>0.745482</td>\n",
              "      <td>0.970730</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.043448</td>\n",
              "      <td>0.999922</td>\n",
              "      <td>0.812166</td>\n",
              "      <td>0.977857</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128, 48, 48])\n",
            "torch.Size([1, 128, 48, 48])\n",
            "torch.Size([1, 128, 48, 48])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "[0.812166] 0.8121659159660339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3311lHhYs7xr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "088f6d92-8888-40fc-c85e-a094b1a38e18"
      },
      "source": [
        "epochs = [80]\n",
        "for e in epochs:\n",
        "    mixup=0 if e<20 else 0.2\n",
        "    learn = get_learn(model=model, size=192, bs=32, mixup=mixup)\n",
        "    learn.fit_fc(e, lr=4e-3, moms=(0.95,0.95), start_pct=0.72)\n",
        "    res += [learn.recorder.metrics[-1][0].item()]\n",
        "print([round(x, 6) for x in res], sum(res)/len(res))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data path   /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learn path /root/.fastai/data/imagewoof2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='72' class='' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      90.00% [72/80 3:08:12<20:54]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>top_k_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.007729</td>\n",
              "      <td>1.773068</td>\n",
              "      <td>0.446679</td>\n",
              "      <td>0.878595</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.806054</td>\n",
              "      <td>1.570455</td>\n",
              "      <td>0.538814</td>\n",
              "      <td>0.920590</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.646740</td>\n",
              "      <td>1.379984</td>\n",
              "      <td>0.619242</td>\n",
              "      <td>0.946806</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.536034</td>\n",
              "      <td>1.236077</td>\n",
              "      <td>0.700178</td>\n",
              "      <td>0.959023</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.437029</td>\n",
              "      <td>1.120115</td>\n",
              "      <td>0.750318</td>\n",
              "      <td>0.967676</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.374157</td>\n",
              "      <td>1.117547</td>\n",
              "      <td>0.751845</td>\n",
              "      <td>0.969967</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.345236</td>\n",
              "      <td>1.069032</td>\n",
              "      <td>0.770171</td>\n",
              "      <td>0.977857</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.282645</td>\n",
              "      <td>1.020498</td>\n",
              "      <td>0.796131</td>\n",
              "      <td>0.973021</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.250353</td>\n",
              "      <td>1.004452</td>\n",
              "      <td>0.798167</td>\n",
              "      <td>0.978621</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.209319</td>\n",
              "      <td>0.939064</td>\n",
              "      <td>0.826164</td>\n",
              "      <td>0.982184</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.195975</td>\n",
              "      <td>0.978047</td>\n",
              "      <td>0.810130</td>\n",
              "      <td>0.977857</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.172745</td>\n",
              "      <td>0.941770</td>\n",
              "      <td>0.820056</td>\n",
              "      <td>0.979130</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.130309</td>\n",
              "      <td>0.931612</td>\n",
              "      <td>0.833800</td>\n",
              "      <td>0.978112</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.095639</td>\n",
              "      <td>0.906328</td>\n",
              "      <td>0.835582</td>\n",
              "      <td>0.981675</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.089046</td>\n",
              "      <td>0.918505</td>\n",
              "      <td>0.834309</td>\n",
              "      <td>0.979384</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.086303</td>\n",
              "      <td>0.890505</td>\n",
              "      <td>0.851616</td>\n",
              "      <td>0.981420</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.065117</td>\n",
              "      <td>0.901537</td>\n",
              "      <td>0.846780</td>\n",
              "      <td>0.977348</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.027332</td>\n",
              "      <td>0.888132</td>\n",
              "      <td>0.845508</td>\n",
              "      <td>0.981929</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.025235</td>\n",
              "      <td>0.886231</td>\n",
              "      <td>0.851871</td>\n",
              "      <td>0.980911</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.989736</td>\n",
              "      <td>0.876827</td>\n",
              "      <td>0.857725</td>\n",
              "      <td>0.981675</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.003416</td>\n",
              "      <td>0.887660</td>\n",
              "      <td>0.852889</td>\n",
              "      <td>0.981166</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.988149</td>\n",
              "      <td>0.854175</td>\n",
              "      <td>0.865869</td>\n",
              "      <td>0.983965</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.987004</td>\n",
              "      <td>0.856258</td>\n",
              "      <td>0.866124</td>\n",
              "      <td>0.983711</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.975805</td>\n",
              "      <td>0.861259</td>\n",
              "      <td>0.857470</td>\n",
              "      <td>0.982184</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.959824</td>\n",
              "      <td>0.862891</td>\n",
              "      <td>0.861797</td>\n",
              "      <td>0.978366</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.969858</td>\n",
              "      <td>0.866880</td>\n",
              "      <td>0.856452</td>\n",
              "      <td>0.982438</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.954756</td>\n",
              "      <td>0.853932</td>\n",
              "      <td>0.866633</td>\n",
              "      <td>0.982947</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.951583</td>\n",
              "      <td>0.841389</td>\n",
              "      <td>0.864597</td>\n",
              "      <td>0.982438</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.945166</td>\n",
              "      <td>0.822340</td>\n",
              "      <td>0.879104</td>\n",
              "      <td>0.980911</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.923631</td>\n",
              "      <td>0.858977</td>\n",
              "      <td>0.861288</td>\n",
              "      <td>0.976839</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.937565</td>\n",
              "      <td>0.865295</td>\n",
              "      <td>0.859252</td>\n",
              "      <td>0.979130</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.912987</td>\n",
              "      <td>0.844804</td>\n",
              "      <td>0.867905</td>\n",
              "      <td>0.981420</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.934914</td>\n",
              "      <td>0.857842</td>\n",
              "      <td>0.865869</td>\n",
              "      <td>0.980911</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.927799</td>\n",
              "      <td>0.861994</td>\n",
              "      <td>0.854416</td>\n",
              "      <td>0.980148</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.921949</td>\n",
              "      <td>0.866654</td>\n",
              "      <td>0.857470</td>\n",
              "      <td>0.978621</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.915236</td>\n",
              "      <td>0.839571</td>\n",
              "      <td>0.862815</td>\n",
              "      <td>0.978875</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.905987</td>\n",
              "      <td>0.861332</td>\n",
              "      <td>0.868923</td>\n",
              "      <td>0.976075</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.887818</td>\n",
              "      <td>0.827936</td>\n",
              "      <td>0.873759</td>\n",
              "      <td>0.981675</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.901386</td>\n",
              "      <td>0.839943</td>\n",
              "      <td>0.868923</td>\n",
              "      <td>0.981420</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.888210</td>\n",
              "      <td>0.822874</td>\n",
              "      <td>0.876304</td>\n",
              "      <td>0.979639</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.891680</td>\n",
              "      <td>0.839120</td>\n",
              "      <td>0.873250</td>\n",
              "      <td>0.974803</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.896207</td>\n",
              "      <td>0.862914</td>\n",
              "      <td>0.861033</td>\n",
              "      <td>0.979893</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.892109</td>\n",
              "      <td>0.832085</td>\n",
              "      <td>0.862815</td>\n",
              "      <td>0.984729</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.888155</td>\n",
              "      <td>0.822705</td>\n",
              "      <td>0.872232</td>\n",
              "      <td>0.980148</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.883418</td>\n",
              "      <td>0.830546</td>\n",
              "      <td>0.867142</td>\n",
              "      <td>0.982693</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.875584</td>\n",
              "      <td>0.846704</td>\n",
              "      <td>0.867651</td>\n",
              "      <td>0.979639</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.888625</td>\n",
              "      <td>0.839071</td>\n",
              "      <td>0.864088</td>\n",
              "      <td>0.982693</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.884882</td>\n",
              "      <td>0.838205</td>\n",
              "      <td>0.872741</td>\n",
              "      <td>0.978621</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.881685</td>\n",
              "      <td>0.836981</td>\n",
              "      <td>0.864342</td>\n",
              "      <td>0.980148</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.891176</td>\n",
              "      <td>0.858151</td>\n",
              "      <td>0.859506</td>\n",
              "      <td>0.980402</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.876814</td>\n",
              "      <td>0.805302</td>\n",
              "      <td>0.882922</td>\n",
              "      <td>0.983711</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.865467</td>\n",
              "      <td>0.840561</td>\n",
              "      <td>0.866633</td>\n",
              "      <td>0.980402</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.870665</td>\n",
              "      <td>0.835833</td>\n",
              "      <td>0.869178</td>\n",
              "      <td>0.980402</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.873948</td>\n",
              "      <td>0.841073</td>\n",
              "      <td>0.866633</td>\n",
              "      <td>0.981420</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.867149</td>\n",
              "      <td>0.847199</td>\n",
              "      <td>0.868669</td>\n",
              "      <td>0.978621</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.892588</td>\n",
              "      <td>0.853867</td>\n",
              "      <td>0.866887</td>\n",
              "      <td>0.982947</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.876219</td>\n",
              "      <td>0.821533</td>\n",
              "      <td>0.877577</td>\n",
              "      <td>0.981675</td>\n",
              "      <td>02:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.864130</td>\n",
              "      <td>0.834649</td>\n",
              "      <td>0.871978</td>\n",
              "      <td>0.981420</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.884094</td>\n",
              "      <td>0.830959</td>\n",
              "      <td>0.870705</td>\n",
              "      <td>0.981675</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.868452</td>\n",
              "      <td>0.833929</td>\n",
              "      <td>0.871214</td>\n",
              "      <td>0.978112</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.863704</td>\n",
              "      <td>0.830437</td>\n",
              "      <td>0.868669</td>\n",
              "      <td>0.984729</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.842574</td>\n",
              "      <td>0.822402</td>\n",
              "      <td>0.871978</td>\n",
              "      <td>0.981675</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.857152</td>\n",
              "      <td>0.827246</td>\n",
              "      <td>0.876050</td>\n",
              "      <td>0.978875</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.856730</td>\n",
              "      <td>0.817818</td>\n",
              "      <td>0.881140</td>\n",
              "      <td>0.978875</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.840294</td>\n",
              "      <td>0.806478</td>\n",
              "      <td>0.885213</td>\n",
              "      <td>0.982438</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.832681</td>\n",
              "      <td>0.810716</td>\n",
              "      <td>0.877577</td>\n",
              "      <td>0.982184</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.830579</td>\n",
              "      <td>0.805541</td>\n",
              "      <td>0.882922</td>\n",
              "      <td>0.983965</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.840477</td>\n",
              "      <td>0.800596</td>\n",
              "      <td>0.876050</td>\n",
              "      <td>0.982947</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.830729</td>\n",
              "      <td>0.791133</td>\n",
              "      <td>0.889285</td>\n",
              "      <td>0.980402</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.820089</td>\n",
              "      <td>0.794680</td>\n",
              "      <td>0.888776</td>\n",
              "      <td>0.979384</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.806938</td>\n",
              "      <td>0.785927</td>\n",
              "      <td>0.886485</td>\n",
              "      <td>0.984474</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.813374</td>\n",
              "      <td>0.772685</td>\n",
              "      <td>0.896666</td>\n",
              "      <td>0.983202</td>\n",
              "      <td>02:36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='87' class='' max='282' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      30.85% [87/282 00:39<01:28 0.7975]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 128, 48, 48])\n",
            "torch.Size([1, 128, 48, 48])\n",
            "torch.Size([1, 128, 48, 48])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 256, 24, 24])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n",
            "torch.Size([1, 512, 12, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKN__HksdGQn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "111f8b86-1415-40e1-fabe-65432cdc9146"
      },
      "source": [
        "res"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c08785e04264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EZg0C8yyxVu",
        "colab_type": "text"
      },
      "source": [
        "conv: fully, conv1x1: fully\n",
        "\n",
        "in 5, 5, 5, 20, and 80 epochs:\n",
        "\n",
        "`[0.795113, 0.78646, 0.792314, 0.885722, 0.899211]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzJXr1BwHbdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(80):\n",
        "    print('epoch {} {}'.format(i, learn.recorder.metrics[i][0].item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXweLcylmmvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}